name: starlake

services:
  starlake-nas:
    image: starlakeai/starlake-nas:latest
    build:
      context: .  # Assuming Dockerfile_nas is in the current directory
      dockerfile: Dockerfile_nas
    container_name: starlake-nas
    restart: on-failure
    privileged: true  # Required to access /proc/fs/nfsd
    healthcheck:
      test: ["CMD", "/healthcheck.sh"]
      interval: "1s"
    volumes:
      - projects_data:/projects
      - external_projects_data:/external_projects
      # use local mount if you want such as
      # - starlake-prj-nfs-mount:/external_projects

  starlake-db:
    image: postgres:17
    restart: on-failure
    container_name: starlake-db
    ports:
      - ${SL_DB_PORT:-5432}:5432
    environment:
      POSTGRES_USER: ${SL_POSTGRES_USER:-dbuser}
      POSTGRES_PASSWORD: ${SL_POSTGRES_PASSWORD:-dbuser123}
      POSTGRES_DB: ${SL_POSTGRES_DB:-starlake}
      AIRFLOW_DB: ${AIRFLOW_DB:-airflow}
      DAGSTER_DB: ${DAGSTER_DB:-dagster}
    command: postgres -c 'config_file=/etc/postgresql/postgresql.conf'
    healthcheck:
      test: ["CMD", "pg_isready", "-q", "-U", "${SL_POSTGRES_USER:-dbuser}", "-d", "${SL_POSTGRES_DB:-starlake}"]
      interval: "1s"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./conf/postgres/postgresql.conf:/etc/postgresql/postgresql.conf
      - ./scripts/airflow/init-database.sh:/docker-entrypoint-initdb.d/init-airflow-database.sh
      - ./scripts/dagster/init-database.sh:/docker-entrypoint-initdb.d/init-dagster-database.sh

  starlake-init-airflow-db:
    image: starlakeai/starlake-airflow:latest
    restart: on-failure
    build:
      context: .  # Assuming Dockerfile_airflow is in the current directory
      dockerfile: Dockerfile_airflow
    container_name: starlake-init-airflow-db
    depends_on:
      starlake-db:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${SL_POSTGRES_USER:-dbuser}:${SL_POSTGRES_PASSWORD:-dbuser123}@starlake-db:5432/${AIRFLOW_DB:-airflow}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      INSTALL_MYSQL_CLIENT: 'false'
      INSTALL_MSSQL_CLIENT: 'false'
    entrypoint: >
      /bin/bash -c "
      sleep 10 &&
      airflow db init &&
      airflow db migrate &&
      airflow users create --username ${AIRFLOW_USERNAME:-admin} --firstname ${AIRFLOW_FIRSTNAME:-Admin} --lastname ${AIRFLOW_LASTNAME:-User} --role Admin --email ${AIRFLOW_EMAIL:-admin@example.com} --password ${AIRFLOW_PASSWORD:-admin}"

  starlake-airflow:
    image: starlakeai/starlake-airflow:latest
    build:
      context: .  # Assuming Dockerfile_airflow is in the current directory
      dockerfile: Dockerfile_airflow
    container_name: starlake-airflow
    restart: on-failure
    depends_on:
      starlake-db:
        condition: service_healthy
      starlake-nas:
        condition: service_healthy
      starlake-init-airflow-db:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-s", "-I", "http://starlake-airflow:8080/airflow/health"]
      interval: "5s"
      retries: 60
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor # SequentialExecutor is required to load files sequentially - to use LocalExecutor we have to fix the creation of audit tables concurrently while loading several tables in parallel for the first time
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${SL_POSTGRES_USER:-dbuser}:${SL_POSTGRES_PASSWORD:-dbuser123}@starlake-db:5432/${AIRFLOW_DB:-airflow}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      INSTALL_MYSQL_CLIENT: 'false'
      INSTALL_MSSQL_CLIENT: 'false'
      SL_HOME: /app/starlake
      AIRFLOW__WEBSERVER__BASE_URL: http://starlake-airflow:8080/airflow
      AIRFLOW__CORE__LOGGING_LEVEL: WARN
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
    entrypoint: >
      /bin/bash -c "
      sleep 10 &&
      pip install --no-cache-dir starlake-orchestration[airflow] --upgrade  --force-reinstall &&
      sudo mkdir -p /mnt/filestore/projects &&
      sudo mkdir -p /mnt/filestore/external_projects &&
      sudo mount -v -o nolock starlake-nas:/projects /mnt/filestore/projects &&
      sudo mount -v -o nolock starlake-nas:/projects/dags /opt/airflow/dags &&
      sudo mount -v -o nolock starlake-nas:/external_projects /mnt/filestore/external_projects &&
      airflow scheduler &
      exec airflow webserver"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock  # Mount Docker socket to run Docker commands from the container
      - airflow_logs:/opt/airflow/logs
    privileged: true  # Required for mounting NFS

  starlake-api:
    image: starlakeai/starlake-1.3-api:${SL_API_VERSION:-0.1}
    pull_policy: always
    container_name: starlake-api
    restart: on-failure
    depends_on:
      starlake-db:
        condition: service_healthy
      starlake-nas:
        condition: service_healthy
      starlake-airflow:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-s", "-I", "http://starlake-api:9000/api/v1/health" ]
      interval: "5s"
      retries: 60
    privileged: true  # Required for mount permissions
    environment:
      - SL_HOME=/app/starlake
      - SL_FS=file://
      - SL_ENV=
      - SL_ROOT=
      - SL_USE_LOCAL_FILE_SYSTEM=false
      - SL_API_GIT_COMMAND_ROOT=/git
      - SL_API_SECURE=false
      - SL_API_SESSION_AS_HEADER=true
      - SL_API_HTTP_FRONT_URL=${SL_API_HTTP_FRONT_URL:-http://starlake-ui}
      - SL_API_HTTP_INTERFACE=0.0.0.0
      - SL_API_HTTP_PORT=9000
      - SL_LOG_LEVEL=${SL_LOG_LEVEL:-info}
      - SL_API_JDBC_DRIVER=org.postgresql.Driver
      - SL_API_JDBC_USER=${SL_POSTGRES_USER:-dbuser}
      - SL_API_JDBC_PASSWORD=${SL_POSTGRES_PASSWORD:-dbuser123}
      - SL_API_JDBC_URL=jdbc:postgresql://starlake-db:5432/${SL_POSTGRES_DB:-starlake}?user=${SL_POSTGRES_USER:-dbuser}&password=${SL_POSTGRES_PASSWORD:-dbuser123} # JDBC URL to connect to the database
      - SL_API_DOMAIN=${SL_API_DOMAIN:-localhost}
      - SL_API_PROJECT_ROOT=/mnt/filestore/projects
      - SL_API_ORCHESTRATOR_PRIVATE_URL=http://localhost:${SL_UI_PORT:-80}/airflow/
      - SL_API_AIRFLOW_PRIVATE_URL=http://starlake-airflow:8080/airflow/
      - ENVIRONMENT=local # local environment
      - FILESTORE_SHARE_NAME=projects  # Environment variable to specify the share name of the NAS
      - FILESTORE_IP_ADDRESS=starlake-nas  # Environment variable to specify the IP address of the NAS
      - FILESTORE_MNT_DIR=/mnt/filestore/projects  # Environment variable to specify the mount path inside starlake-api container
      - FILESTORE_EXTERNAL_PROJECTS_SHARE_NAME=external_projects
      - POSTGRES_HOST=starlake-db
      - POSTGRES_DB=${SL_POSTGRES_DB:-starlake}
      - POSTGRES_USER=${SL_POSTGRES_USER:-dbuser}
      - POSTGRES_PASSWORD=${SL_POSTGRES_PASSWORD:-dbuser123}
      - SL_UI_DEMO=${SL_UI_DEMO:-false}
      - SL_API_MAIL_HOST=${SL_API_MAIL_HOST:-smtp.sendgrid.net}
      - SL_API_MAIL_PORT=${SL_API_MAIL_PORT:-587}
      - SL_API_MAIL_USER=${SL_API_MAIL_USER:-apikey}
      - SL_API_MAIL_PASSWORD=${SL_API_MAIL_PASSWORD:-}
      - SL_API_MAIL_FROM=${SL_API_MAIL_FROM:-contact@starlake.ai}
      - SL_API_MAX_USER_SPACE_MB=${SL_API_MAX_USER_SPACE_MB:-0}
      - _JAVA_OPTIONS=-XX:UseSVE=0
    entrypoint: >
      /bin/bash -c "
      sleep 10 &&
      python3 -m pip install --break-system-packages --no-cache-dir starlake-orchestration[airflow] --upgrade &&
      /usr/bin/tini -- /app/run-api.sh"

  starlake-projects:
    image: starlakeai/starlake-projects:latest
    build:
      context: .  # Assuming Dockerfile_projects is in the current directory
      dockerfile: Dockerfile_projects
    container_name: starlake-projects
    restart: on-failure
    depends_on:
      starlake-db:
        condition: service_healthy
      starlake-api:
        condition: service_healthy
    privileged: true  # Required for mount permissions
    environment:
      POSTGRES_USER: ${SL_POSTGRES_USER:-dbuser}
      POSTGRES_PASSWORD: ${SL_POSTGRES_PASSWORD:-dbuser123}
      POSTGRES_DB: ${SL_POSTGRES_DB:-starlake}
      POSTGRES_HOST: starlake-db
      FILESTORE_SHARE_NAME: projects  # Environment variable to specify the share name of the NAS
      FILESTORE_IP_ADDRESS: starlake-nas  # Environment variable to specify the IP address of the NAS
      FILESTORE_MNT_DIR: /mnt/filestore/projects  # Environment variable to specify the mount path inside starlake-api container
    volumes:
      - ./projects:/projects

  starlake-ui:
    image: starlakeai/starlake-1.3-ui:${SL_UI_VERSION:-0.1}
    pull_policy: always
    container_name: starlake-ui
    restart: on-failure
    depends_on:
      starlake-api:
        condition: service_healthy
      starlake-projects:
        condition: service_completed_successfully
    privileged: true  # Required for mount permissions
    ports:
      - ${SL_UI_PORT:-80}:80  # starlake-ui default port
    environment:
      - FILESTORE_SHARE_NAME=projects  # Environment variable to specify the share name of the NAS
      - FILESTORE_IP_ADDRESS=starlake-nas  # Environment variable to specify the IP address of the NAS
      - FILESTORE_MNT_DIR=/mnt/filestore/projects  # Environment variable to specify the mount path inside starlake-api container
    volumes:
      - .env.docker:/app/.env:ro

volumes:
  projects_data:
  pgdata:
  airflow_logs:
  external_projects_data:
  # starlake-prj-nfs-mount:
  #   driver: local
  #   driver_opts:
  #     type: nfs
  #     o: addr=host.docker.internal,rw,nolock,hard,nointr,nfsvers=3
  #     device: ":/path/to/starlake/project/container"
